{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "35ba1335",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.utils.data as td\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "import dask.dataframe as dd\n",
    "\n",
    "froot = './data/k562_main'\n",
    "\n",
    "df = dd.read_csv(froot + \".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "25c67eaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   seqnames    start      end strand  ensembl_gene_id  score      ctcf  \\\n",
      "0         1  3859709  3859709      +  ENSG00000169598    0.0 -0.010876   \n",
      "1         1  3859710  3859710      +  ENSG00000169598    0.0 -0.010887   \n",
      "2         1  3859711  3859711      +  ENSG00000169598    0.0 -0.010902   \n",
      "3         1  3859712  3859712      +  ENSG00000169598    0.0 -0.010920   \n",
      "4         1  3859713  3859713      +  ENSG00000169598    0.0 -0.010941   \n",
      "\n",
      "   h3k36me3   h3k4me1  h3k79me2  ...       sj3       dms      rpts  wgbs  \\\n",
      "0  0.353765 -0.078256 -0.156547  ... -0.057178 -0.307549  0.249626   0.0   \n",
      "1  0.347003 -0.077117 -0.155891  ... -0.057178 -0.307549  0.249626   0.0   \n",
      "2  0.340295 -0.075994 -0.155236  ... -0.057178 -0.307549  0.249626   0.0   \n",
      "3  0.333641 -0.074887 -0.154583  ... -0.057178 -0.307549  0.249626   0.0   \n",
      "4  0.327043 -0.073795 -0.153930  ... -0.057178 -0.307549  0.249626   0.0   \n",
      "\n",
      "       A         T         G         C  lambda_alphaj      zeta  \n",
      "0 -0.625 -0.678443  1.954571 -0.494439       0.014335  0.932859  \n",
      "1 -0.625 -0.678443  1.954571 -0.494439       0.014335  0.933402  \n",
      "2 -0.625  1.473964 -0.511621 -0.494439       0.014335  0.933944  \n",
      "3 -0.625  1.473964 -0.511621 -0.494439       0.014335  0.934485  \n",
      "4 -0.625 -0.678443  1.954571 -0.494439       0.014335  0.935024  \n",
      "\n",
      "[5 rows x 24 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7b54d1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df = df.groupby('ensembl_gene_id')\n",
    "\n",
    "group_lengths = grouped_df.size().reset_index().rename(columns={0: 'gene_length'})\n",
    "df = df.merge(group_lengths, on=\"ensembl_gene_id\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9a8abcdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ctcf', 'h3k36me3', 'h3k4me1', 'h3k79me2', 'h3k9me1', 'h3k9me3', 'h4k20me1', 'sj5', 'sj3', 'dms', 'rpts', 'wgbs']\n",
      "['A', 'T', 'G', 'C']\n"
     ]
    }
   ],
   "source": [
    "column_names = df.columns.tolist()\n",
    "feature_names = column_names[6:-7]\n",
    "nucleotides = column_names[-7:-3]\n",
    "print(feature_names)\n",
    "print(nucleotides)\n",
    "\n",
    "# process read counts\n",
    "X_ji_df = grouped_df['score'].apply(list, meta=('score', 'object'))\n",
    "\n",
    "# process GLM simulated elongation rates\n",
    "Z_ji_df = grouped_df['zeta'].apply(list, meta=('zeta', 'object'))\n",
    "\n",
    "X_ji = X_ji_df.to_dask_array()\n",
    "Z_ji = Z_ji_df.to_dask_array()\n",
    "\n",
    "\n",
    "\n",
    "#num_samples = len(X_ji)\n",
    "\n",
    "#X_ji_result = X_ji.compute()\n",
    "#Z_ji_result = Z_ji.compute()\n",
    "\n",
    "#X_ji_list = X_ji_result.tolist()\n",
    "#Z_ji_list = Z_ji_result.tolist()\n",
    "\n",
    "#print(len(X_ji_list))\n",
    "\n",
    "#print(num_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e396e7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "#Y_ji is a list of samples containing lists of their feature values\n",
    "    # [   \n",
    "    #   [[sample_1_feature_1], [sample_1_feature_2], [sample_1_feature_3]],\n",
    "    #   [[sample_2_feature_1], [sample_1_feature_2], [sample_1_feature_3]],  \n",
    "    # ]\n",
    "\n",
    "Y_ji_df = grouped_df[feature_names].apply(list, meta=('features', 'object'))\n",
    "\n",
    "Y_ji = Y_ji_df.to_dask_array()\n",
    "\n",
    "Y_ji_arr = Y_ji.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c660a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize features\n",
    "scaler = MinMaxScaler()\n",
    "Y_ji = np.array(Y_ji)\n",
    "# reshape dataset to [num_samples, num_features * feature_length]\n",
    "Y_ji_reshaped = Y_ji.reshape(Y_ji.shape[0], -1)\n",
    "normalized_Y_ji_reshaped = scaler.fit_transform(Y_ji_reshaped)\n",
    "Y_ji = normalized_Y_ji_reshaped.reshape(Y_ji.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d504b303",
   "metadata": {},
   "outputs": [],
   "source": [
    "C_j = df.groupby('ensembl_gene_id')['lambda_alphaj'].apply(list).tolist() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b56dcc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate_fn(batch):\n",
    "    # Group samples based on their lengths\n",
    "    grouped_samples = {}\n",
    "    for sample in batch:\n",
    "        sample_length = len(sample[0])  # All elements in a sample should have the same length\n",
    "        if sample_length not in grouped_samples:\n",
    "            grouped_samples[sample_length] = []\n",
    "        grouped_samples[sample_length].append(sample)\n",
    "\n",
    "    # Create batches for each group\n",
    "    batches = []\n",
    "    for sample_length, samples in grouped_samples.items():\n",
    "        batch_data = [torch.stack(items) for items in zip(*samples)]\n",
    "        batches.append(batch_data)\n",
    "\n",
    "    return batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f18812",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, Y_ji, X_ji, C_j, Z_ji):\n",
    "        self.Y_ji = Y_ji\n",
    "        self.X_ji = X_ji\n",
    "        self.C_j = C_j\n",
    "        self.Z_ji = Z_ji\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X_ji)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'Y_ji':  torch.tensor(self.Y_ji[idx], dtype=torch.float32),\n",
    "            'X_ji': torch.tensor(self.X_ji[idx], dtype=torch.float32),\n",
    "            'C_j': torch.tensor(self.C_j[idx], dtype=torch.float32),\n",
    "            'Z_ji': torch.tensor(self.Z_ji[idx], dtype=torch.float32)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5977e736",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CustomDataset(Y_ji, X_ji, C_j, Z_ji)\n",
    "\n",
    "trnset, valset, tstset = td.random_split(dataset, [0.5,0.25,0.25])\n",
    "\n",
    "trndl = DataLoader(trnset, batch_size=1, shuffle=True)\n",
    "tstdl = DataLoader(tstset, batch_size=1, shuffle=False)\n",
    "valdl = DataLoader(valset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2404f7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(Model, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        outputs, _ = self.lstm(inputs)\n",
    "        averaged_outputs = torch.mean(outputs, dim=1)\n",
    "        predictions = self.fc(averaged_outputs)\n",
    "        return predictions\n",
    "\n",
    "# input size: [50, 12, 2000]\n",
    "input_size = 2000\n",
    "hidden_size = 32\n",
    "output_size = 2000\n",
    "\n",
    "model = Model(input_size, hidden_size, output_size)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03d0d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomLoss, self).__init__()\n",
    "\n",
    "    def forward(self, X_ji, C_j, Z_ji):\n",
    "        epsilon = 1e-8\n",
    "        clipped_Z_ji = torch.clamp(Z_ji, epsilon)\n",
    "        loss = X_ji * torch.log(clipped_Z_ji) + C_j * torch.exp(-clipped_Z_ji)\n",
    "        # compute mean over batch to normalize due to varying batch sizes\n",
    "        return loss.mean() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c28ed03",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 100\n",
    "\n",
    "loss_fn = CustomLoss()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in trndl:\n",
    "        optimizer.zero_grad()\n",
    "        Y_ji_batch = batch['Y_ji']\n",
    "        X_ji_batch = batch['X_ji']\n",
    "        C_j_batch = batch['C_j']\n",
    "        outputs = model(Y_ji_batch)\n",
    "        loss = loss_fn(X_ji_batch, C_j_batch, outputs)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94841213",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "epochs = range(1, len(hist[0]) + 1)\n",
    "plt.plot(epochs, hist[0], label='train_loss')\n",
    "plt.plot(epochs, hist[1], label='valid_loss')\n",
    "\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9756b162",
   "metadata": {},
   "outputs": [],
   "source": [
    "mae = []\n",
    "mse = []\n",
    "for inputs, labels in tstdl:\n",
    "    #interpret_model(inputs, labels)\n",
    "    outputs = model(inputs)\n",
    "    mae.append(torch.mean(torch.abs(outputs - labels), dim=0))\n",
    "    mse.append(torch.mean((outputs - labels)**2, dim=0))\n",
    "\n",
    "mean_mae = torch.mean(torch.stack(mae))    \n",
    "mean_mse = torch.mean(torch.stack(mse))\n",
    "print(\"Overall Mean Absolute Error (MAE):\", round(mean_mae.item(), 3))\n",
    "print(\"Overall Mean Squared Error (MSE):\", round(mean_mse.item(), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7239c5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "inputs = next(iter(tstdl)) \n",
    "print(\"number of samples: \" + str(len(inputs)))\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(inputs['Y_ji'])\n",
    "    \n",
    "print(outputs)\n",
    "\n",
    "targets = inputs['Z_ji']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9f9c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(targets.shape)\n",
    "print(outputs.shape)\n",
    "print(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c204b7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_points = 2000\n",
    "indices = np.arange(num_points)\n",
    "\n",
    "# Subset the data for indices 0 to 200\n",
    "subset_indices = indices[:201]  # 0 to 200\n",
    "subset_outputs = outputs[:, :201]  # Select the first 201 points for all samples\n",
    "\n",
    "fig, axs = plt.subplots(3, 3, figsize=(15, 15))\n",
    "\n",
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "        axs[i, j].plot(subset_indices, subset_outputs[i + j * 3])\n",
    "        axs[i, j].set_ylim(-1, 2)\n",
    "        axs[i, j].set_xlabel('Index')\n",
    "        axs[i, j].set_ylabel('Elongation Rates')\n",
    "        axs[i, j].set_title(f'Plot {i + j * 3 + 1}')\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884be6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = range(len(targets[0]))\n",
    "\n",
    "fig, axs = plt.subplots(3, 3, figsize=(15,15))\n",
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "        axs[i, j].scatter(indices, targets[i+j*3], s=5)\n",
    "        axs[i, j].scatter(indices, outputs[i+j*3], s=5)\n",
    "        axs[i, j].set_ylim(-1, 2)\n",
    "\n",
    "plt.xlabel('Index')\n",
    "plt.ylabel('Elongation Rates')\n",
    "plt.legend(['GLM Elongation Rate', 'NN Elongation Rate'], loc='upper center', bbox_to_anchor=(0.5, -0.6))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
